{:title "Going on a tangent"
 :layout :post
 :tags  ["clojure" "regular expressions"]
 :toc true
 :navbar? true}

<!-- # Going on a tangent -->


You ever write a regular expression and then wonder how the heck it determine its matches? how does it go from such a terse syntax like `f[o]*` to a little machine that determines "YES, we're a match" or "NO, but oh well, there's more fish in the sea". 

$`blah`$

Well I did! And I found a particularly nice algorithm for determining if a regular expression matches a given input string. The two reasons I chose this algorithm are:
1. It's very simple to understand, and the code expressing it is very short. 
2. It relies on the concept of the `derivative` of a regular expression! 

The algorithm is simple to state: 
1. if the current string we're matching is empty (written $\epsilon$), and the regular expression we want to match against matches the empty string, then $s$ matches the regex $R$! Otherwise, it does not match. 
2. If the string is non-empty, then we perform two replacements + run again: 
    - the current string is replaced by the the current string without the first character (ex: $`blue \rightarrow lue`$)
    - the current regex is replaced by the **derivative** of $R$ with respect to the first character in $s$ (ex: $R \rightarrow D_b(R)$, since $b$ is the first character in $blue$). 

Ok, so what's this mysterious derivative? 

In short: it is the regular expression that matches the same stuff as $R$, assuming that a given character (ex: $b$) has **already been matched** by $R$. You can think of it as the "tail" of the regular expression. 

## Primer on regular expressions

Given: 
- a string `$s$` to match, say "blue"
- a regular expression $R$ that will compute a match, say $b(l|o)u(e|t)(s)*$
    - recall: the $|$ operator is alternation (aka "choice"), and $*$ is iteration. The above regex could match $bout$, $blue$, $blot$, $boue$, as well as all of these variations ending in ANY number of $s$'s (ex: $bluessssssss$).
    - In addition to all those operations, we are doing one other sneaky one: concatenating several regexes together to form $R$! The characters $b$, $l$, $o$, $u$, $e$, $t$ are each very simple regexes that ONLY match that one character, and we can concat them together to form a regex that matches the first thing, then the second thing, then the third thing, etc.! 

then we want to determine if $s$ is recognized by $R$. To do this, we can slightly more formally define $R$ as representing the **set of all the strings** (drawn from a particular alphabet) **that it matches**: 
$$
    R := \{ \text{ strings t } | \text{ t is matched by R and } t \in \Sigma \text{ (our alphabet)} \}
$$. 

Example (where $\Sigma = \{a,b,c,d,e,...,z\}$):
$$

b(l|o)u(e|t)(s)* = \{
    bout, blue, blot, boue, bouts, blues, blots, boues, boutss, bluess, blotss, bouess, boutsss, bluesss, blotsss, bouesss, ... 
    \}

$$

We will also need a notion of the **empty** regex $\empty$, which is the regex that matches nothing: 

$$
    \empty := \text{the empty set, confusingly also written } \empty \
$$

This is distinct from the regex that matches the empty string $\epsilon$. This regular expression is also written as $\epsilon$: 

$$
    \epsilon := \{ \epsilon \} \
$$

## The Brzozowski derivative

With this more concrete definition of a regex, then the **Brzozowski derivative** of $R$ with respect to a character $a$ is 

$$ 
    D_a(R) := \{\text{ } b \text{ } | \text{ } ab \in R \text{ }\}
$$

In other words, it is the regex that matches everything $R$ would have if $a$ had already been matched.

To compute this, Brzozowski came up with a recursive way of expressing $D_a(R)$: 

$$D_a(\empty) = \empty$$
$$D_a(\epsilon) = \empty$$
$$D_a(a) = \epsilon $$
$$D_a(a') = \empty \text{ if } a \neq a' $$
$$D_a(R_1 R_2) = δ(R_1) D_a(R_2) \text{ }  | \text{ } D_a(R_1) \text{ }  R_2 $$ 
$$D_a(R_1 \text{ } | \text{ }  R_2) = D_a(R_1)\text{ } | \text{ } D_a(R_2)$$
$$D_a(R*) = D_a(R) R*$$

Note that the "product" rule also uses a function $\delta$. This function is a helper, and it just determines if its argument accepts the empty string pattern $\epsilon$. If it doesn't, then it returns the null pattern $\empty$: 

$$\delta(\empty) = \empty$$
$$\delta(\epsilon) = \epsilon$$
$$\delta(a) = \empty$$
$$\delta(R_1 R_2) = \delta(R_1)\delta(R_2)$$
$$\delta(R_1 \text{ } | \text{ } R_2) =\delta(R_1) \text{ } | \text{ } \delta(R_2) $$
$$\delta(R *) = \epsilon$$


This is a lot of rules, and they are best understood through some examples, so let's get on with that. 

Ex: $R = do(g|t)$, $s = dog$
First we have to apply the product rule, since we have a concatenation of 3 regexes. 

$$
    D_d(do(g|t)) = \delta(d) D_d(o(g|t)) \text{ } | \text{ } D_d(d) \text{ }  (o(g|t))
$$

The RHS of the alternation is simpler, so let's tackle that first: 

$$
    D_d(d) \text{ }  (o(g|t)) = \epsilon \text{ } (o(g|t)) = o(g|t)
$$

On the LHS of the |: 

$$
    \empty \text{ } D_d(o(g|t))
$$



$$
    D_d(do(g|t)) = \delta(d) D_d(o(g|t)) \text{ } | \text{ } D_d(d) \text{ }  (o(g|t))
$$

$$
    D_d(do(g|t)) = o(g|t)
$$
$$
    D_o(o(g|t)) = (g|t)
$$

Oh no! How are we supposed to deal with alternation? 

Brzozowski tells us! 
$$
D_c(re_1 | re_2) = D_c(re_1) | D_c(re_2)
$$

So now $D_g((g|t)) = D_g(g) | D_g(t)$




## Does $D_s(R)$ deserve to be called a derivative?



















Every sufficiently nice function can be approximated by its [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), and in some cases, the function is *exactly* equal to its infinite Taylor series: 
$$
\displaystyle f(x) \approx \sum_{n=0}^\infty\ a_n \frac{(x-b)^n}{n!}
$$
where the coefficients $a_n$ are the n-th order derivatives of $f$, and b is some point where $f$ is n-times differentiable (so that taking the n-th derivative makes sense): 

$$
a_n = \frac{d^k }{dx}f(b)
$$

Working with a truncated Taylor expansion of $f$ instead of $f$ itself is a well-known trick, since it approximates a potentially complicated function by a polynomial. If f returns some value when provided with x, then a Taylor approximation is only ever off by a quantifiable amount. 

But why bother using an approximation? Well, we've all had to do really hairy integrals before (or other complicated operations on functions). Doing any sort of calculations (more derivatives, integration, etc.) when working with a Taylor approximation is way easier, since operations like integration can be performed with ease on polynomials!


Mathematical functions aren't the only thing that can benefit from this perspective! A famous saying in computer science is: 
```
Some people, when confronted with a problem, think
“I know, I'll use regular expressions.”   Now they have two problems.
```
